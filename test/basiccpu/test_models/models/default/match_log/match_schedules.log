
For node
free_var %FunctionVar_0_0: Tensor[(1, 3, 32, 32), int8] /* ty=Tensor[(1, 3, 32, 32), int8] */;
free_var %FunctionVar_0_1: Tensor[(1, 3, 3, 3), int8] /* ty=Tensor[(1, 3, 3, 3), int8] */;
%0 = nn.conv2d(%FunctionVar_0_0, %FunctionVar_0_1, strides=[2, 2], padding=[1, 1, 1, 1], kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(1, 1, 16, 16), int32] */;
free_var %FunctionVar_0_2: Tensor[(1), int32] /* ty=Tensor[(1), int32] */;
%1 = nn.bias_add(%0, %FunctionVar_0_2, axis=0) /* ty=Tensor[(1, 1, 16, 16), int32] */;
nn.relu(%1) /* ty=Tensor[(1, 1, 16, 16), int32] */                    
MATCH found that the best schedule, with expected latency 330225943860 and energy 3010386.48, has the following temporal mapping                    
Block 0
for OX_3 in 0:2:  [load tensor FunctionVar_0_0]  [load tensor FunctionVar_0_0]  [load tensor relu] 
	for OX_2 in 0:2: 
		for OX_1 in 0:2: 
			for OX in 0:2: 
				for OY_4 in 0:2: 
					for OY_3 in 0:2: 
						for OY_2 in 0:2: 
							for OY_1 in 0:2: 
								for FX in 0:3: 
									for FY in 0:3: 
										for C in 0:3: 

